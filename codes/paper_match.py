# ä»åŸç½‘ç«™åŒ¹é…è®ºæ–‡æ¡ç›®ï¼Œå¹¶ä¸‹è½½PDF
# https://www.ruizhang.info/
# https://dblp.org/pid/03/505.html?view=bibtex
# 1. ä»ç½‘ç«™ä¸Šè·å–è®ºæ–‡æ¡ç›®
# 2. ä»æœ¬åœ°bibæ–‡ä»¶(dblp ruizhang003)ä¸­è·å–è®ºæ–‡æ¡ç›®
# 3. åŒ¹é…è®ºæ–‡æ¡ç›®ï¼Œå¹¶ä¸‹è½½PDF,å¹¶åŒ¹é…ä¼šè®®æœŸåˆŠçš„shortname
# 4. ä¿å­˜åŒ¹é…ç»“æœ

import requests
from bs4 import BeautifulSoup
import bibtexparser
import re
import os
from urllib.parse import urljoin
from difflib import SequenceMatcher
from tqdm import tqdm

def load_bib_entries(bib_file):
    with open(bib_file, 'r', encoding='utf-8') as f:
        bib_database = bibtexparser.load(f)
    return bib_database.entries

def download_pdf(url, filename, pdf_dir):
    """
    ä¸‹è½½PDFæ–‡ä»¶ï¼Œå¦‚æœæœ¬åœ°å·²å­˜åœ¨åˆ™è·³è¿‡

    Args:
        url: PDFæ–‡ä»¶çš„URL
        filename: ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        pdf_dir: PDFæ–‡ä»¶ä¿å­˜ç›®å½•

    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸï¼ˆåŒ…æ‹¬å·²å­˜åœ¨çš„æƒ…å†µï¼‰
    """
    if not filename:
        return False

    # æ›¿æ¢æ–‡ä»¶åä¸­çš„æ–œæ ä¸ºä¸‹åˆ’çº¿ï¼Œå¹¶å¤„ç†å·²æœ‰çš„ä¸‹åˆ’çº¿
    filename = filename.replace('\\_', '_')
    local_path = os.path.join(pdf_dir, f"{filename}.pdf")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(local_path):
        print(f"PDF already exists: {filename}.pdf")
        return True

    try:
        response = requests.get(url, timeout=30)
        if response.status_code == 200:
            with open(local_path, 'wb') as f:
                f.write(response.content)
            print(f"Downloaded: {filename}.pdf")
            return True
        else:
            print(f"Failed to download {filename}.pdf: HTTP {response.status_code}")
            return False
    except Exception as e:
        print(f"Error downloading {filename}.pdf: {str(e)}")
        return False

def extract_doi_from_url(url):
    doi_patterns = [
        r'10\.\d{4,}/[-._;()/:\w]+',
        r'doi\.org/(.+)$'
    ]
    for pattern in doi_patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(0)
    return None

def normalize_title(title):
    """
    è§„èŒƒåŒ–æ ‡é¢˜ï¼š
    1. è½¬æ¢ä¸ºå°å†™
    2. åªä¿ç•™å­—æ¯å’Œæ•°å­—ï¼Œå…¶ä»–æ‰€æœ‰å­—ç¬¦éƒ½æ›¿æ¢ä¸ºç©ºæ ¼
    3. åˆ†è¯å¹¶ç”¨å•ä¸ªç©ºæ ¼é‡æ–°è¿æ¥
    """
    if not title:
        return ""

    # è½¬æ¢ä¸ºå°å†™
    title = title.lower()

    # å°†æ‰€æœ‰éå­—æ¯æ•°å­—å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
    # [^a-z0-9] è¡¨ç¤ºåŒ¹é…ä»»ä½•ä¸æ˜¯å°å†™å­—æ¯æˆ–æ•°å­—çš„å­—ç¬¦
    title = re.sub(r'[^a-z0-9]+', ' ', title)

    # åˆ†è¯å¹¶ç”¨å•ä¸ªç©ºæ ¼é‡æ–°è¿æ¥
    words = title.split()
    title = ' '.join(words)

    return title

def fuzzy_match_title(title1, title2, threshold=0.85):
    """
    ä½¿ç”¨è§„èŒƒåŒ–åçš„æ ‡é¢˜è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
    """
    # è§„èŒƒåŒ–ä¸¤ä¸ªæ ‡é¢˜
    norm_title1 = normalize_title(title1)
    norm_title2 = normalize_title(title2)

    # å¦‚æœè§„èŒƒåŒ–åçš„æ ‡é¢˜å®Œå…¨ç›¸åŒ
    if norm_title1 == norm_title2:
        return True

    # ä½¿ç”¨åºåˆ—åŒ¹é…å™¨è®¡ç®—ç›¸ä¼¼åº¦
    similarity = SequenceMatcher(None, norm_title1, norm_title2).ratio()

    # è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    # if similarity > 0.5:  # åªæ‰“å°ç›¸ä¼¼åº¦è¾ƒé«˜çš„åŒ¹é…ç»“æœ
    #     print(f"\nMatching titles:")
    #     print(f"Original 1: {title1}")
    #     print(f"Original 2: {title2}")
    #     print(f"Normalized 1: {norm_title1}")
    #     print(f"Normalized 2: {norm_title2}")
    #     print(f"Similarity: {similarity}")

    return similarity >= threshold

def extract_title_from_font(font_element):
    """
    ä»fontå…ƒç´ ä¸­æå–è®ºæ–‡æ ‡é¢˜ï¼š
    1. å¦‚æœå­˜åœ¨aæ ‡ç­¾ï¼Œä½¿ç”¨aæ ‡ç­¾çš„æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
    2. å¦‚æœä¸å­˜åœ¨aæ ‡ç­¾ï¼Œä½¿ç”¨å€’æ•°ç¬¬äºŒä¸ªé€—å·å’Œæœ€åä¸€ä¸ªé€—å·ä¹‹é—´çš„å†…å®¹ä½œä¸ºæ ‡é¢˜
    """
    if not font_element:
        return None

    # é¦–å…ˆæŸ¥æ‰¾aæ ‡ç­¾
    a_tag = font_element.find('a')
    if a_tag:
        return a_tag.get_text().strip()

    # å¦‚æœæ²¡æœ‰aæ ‡ç­¾ï¼Œä½¿ç”¨é€—å·åˆ†éš”æ³•
    contents = [str(content) for content in font_element.contents]
    text = ''.join(contents)

    # æ‰¾åˆ°æ‰€æœ‰é€—å·çš„ä½ç½®
    comma_positions = [i for i, char in enumerate(text) if char == ',']

    # ç¡®ä¿è‡³å°‘æœ‰ä¸¤ä¸ªé€—å·
    if len(comma_positions) < 2:
        return None

    # è·å–å€’æ•°ç¬¬äºŒä¸ªå’Œæœ€åä¸€ä¸ªé€—å·çš„ä½ç½®
    last_comma = comma_positions[-1]
    second_last_comma = comma_positions[-2]

    # æå–æ ‡é¢˜å¹¶æ¸…ç†
    title = text[second_last_comma + 1:last_comma].strip()

    return title


def write_bibtex_entry(file, entry):
    """å†™å…¥å•ä¸ª BibTeX æ¡ç›®åˆ°æ–‡ä»¶ï¼Œkeyå 15ä¸ªå­—ç¬¦ï¼ˆåŒ…æ‹¬å‰é¢çš„ç©ºæ ¼ï¼‰ï¼Œå·¦å¯¹é½

    Args:
        file: æ–‡ä»¶å¯¹è±¡
        entry: BibTeX æ¡ç›®å­—å…¸
    """
    # è·å–æ¡ç›®ç±»å‹å’ŒID
    entry_type = entry.get('ENTRYTYPE', '')
    entry_id = entry.get('ID', '')
    file.write(f'@{entry_type}{{{entry_id},\n')

    # ä¼˜å…ˆå­—æ®µé¡ºåº
    priority_fields = ['author', 'title']
    # æ–°åŠ çš„å­—æ®µï¼ˆæ”¾åœ¨æœ€åï¼‰
    new_fields = ['pdf', 'shortname']

    def write_field(key, value):
        """å†™å…¥å•ä¸ªå­—æ®µï¼Œkeyå 15ä¸ªå­—ç¬¦ï¼Œå·¦å¯¹é½"""
        # æ ¼å¼åŒ–keyï¼Œç¡®ä¿å 15ä¸ªå­—ç¬¦ï¼Œå·¦å¯¹é½
        formatted_key = f"{key:<15}"  # keyå 15ä¸ªå­—ç¬¦

        if '\n' in str(value):
            # å¯¹å¤šè¡Œå€¼è¿›è¡Œå¤„ç†
            lines = str(value).split('\n')
            formatted_value = lines[0] + '\n' + '\n'.join(' ' * 18 + line for line in lines[1:])
            file.write(f'{formatted_key}= {{{formatted_value}}},\n')
        else:
            file.write(f'{formatted_key}= {{{value}}},\n')

    # å…ˆå†™å…¥ä¼˜å…ˆå­—æ®µ
    for field in priority_fields:
        if field in entry:
            write_field(field, entry[field])

    # å†™å…¥å…¶ä»–åŸæœ‰å­—æ®µ
    for key, value in entry.items():
        if (key not in priority_fields and
            key not in new_fields and
            key not in ['ENTRYTYPE', 'ID']):
            write_field(key, value)

    # æœ€åå†™å…¥æ–°å­—æ®µ
    for field in new_fields:
        if field in entry:
            write_field(field, entry[field])

    file.write('}\n\n')

def replace_special_chars(text):
    """æ›¿æ¢ç‰¹æ®Šå­—ç¬¦"""
    text = text.replace('&nbsp;', ' ')
    text = text.replace('\r', ' ')
    text = text.replace('\n', ' ')
    text = text.replace('\t', ' ')
    text = text.replace('Â¡', '')
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text

def extract_shortname_from_element(text, title):
    """æå–æœŸåˆŠ/ä¼šè®®ä¿¡æ¯ï¼Œä»titleåç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦ï¼ˆé€—å·æˆ–å¥å·ï¼‰å¼€å§‹åˆ°æ¡ç›®ç»“æŸ
    Args:
        text: å®Œæ•´æ–‡æœ¬
        title: è®ºæ–‡æ ‡é¢˜
    Returns:
        str: æå–çš„shortname
    """
    if not title or not text:
        return None

    text = replace_special_chars(text)

    # æ‰¾åˆ°titleåœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
    title_pos = text.find(title)
    if title_pos == -1:
        return None

    # ä»titleåå¼€å§‹æŸ¥æ‰¾ç¬¬ä¸€ä¸ªé€—å·æˆ–å¥å·
    start_pos = title_pos + len(title)
    text_after_title = text[start_pos:]

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦ï¼ˆé€—å·æˆ–å¥å·ï¼‰
    first_separator = None
    for i, char in enumerate(text_after_title):
        if char in [',', '.']:
            first_separator = i
            break

    if first_separator is None:
        return None

    # ä»ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦åå¼€å§‹æå–åˆ°ç»“æŸ
    venue_text = text_after_title[first_separator + 1:].strip()
    if not venue_text:
        return None

    # æŸ¥æ‰¾æ‰€æœ‰æ‹¬å·å†…å®¹
    bracket_matches = re.finditer(r'\((.*?)\)', venue_text)
    for match in bracket_matches:
        bracket_content = match.group(1)
        # å¦‚æœæ‹¬å·å†…å®¹ä¸æ˜¯çº¯æ•°å­—ï¼Œä½¿ç”¨å®ƒä½œä¸ºshortname
        if not bracket_content.isdigit():
            # æ£€æŸ¥æ‹¬å·å†…å®¹æ˜¯å¦å·²åŒ…å«å¹´ä»½
            if re.search(r'\b(19|20)\d{2}\b', bracket_content):
                return bracket_content.strip()
            # å¦‚æœæ²¡æœ‰å¹´ä»½ï¼Œä»æ•´ä¸ªæ–‡æœ¬ä¸­æŸ¥æ‰¾
            year_match = re.search(r'\b(19|20)\d{2}\b', text)
            if year_match:
                return f"{bracket_content.strip()} {year_match.group()}"
            return bracket_content.strip()

    # æ£€æŸ¥å®Œæ•´æ–‡æœ¬æ˜¯å¦å·²åŒ…å«å¹´ä»½
    if re.search(r'\b(19|20)\d{2}\b', venue_text):
        return venue_text.strip()

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°éæ•°å­—çš„æ‹¬å·å†…å®¹ï¼Œè¿”å›å®Œæ•´æ–‡æœ¬ï¼ˆå¦‚æœéœ€è¦æ·»åŠ å¹´ä»½ï¼‰
    year_match = re.search(r'\b(19|20)\d{2}\b', text)
    if year_match:
        return f"{venue_text.strip()} {year_match.group()}"
    return venue_text.strip()

def extract_paperinfo_from_website(pub_element):
    """
    ä»publicationå…ƒç´ ä¸­æå–æ ‡é¢˜å’Œshortnameï¼š
    1. è·å–<li>æ ‡ç­¾çš„å®Œæ•´æ–‡æœ¬ç”¨äºæå–shortname
    2. ä»fontæˆ–spanå…ƒç´ ä¸­æå–æ ‡é¢˜
    """
    full_text = pub_element.get_text(strip=True) # è·å–<li>æ ‡ç­¾çš„å®Œæ•´æ–‡æœ¬

    # é¦–å…ˆå°è¯•ä»fontä¸­æå–æ ‡é¢˜
    font_face = pub_element.find('font')
    if font_face:
        title = extract_title_from_element(font_face)
        if title:
            shortname = extract_shortname_from_element(full_text, title)  # ä½¿ç”¨å®Œæ•´æ–‡æœ¬æå–shortname
            return title, shortname

    # å¦‚æœfontä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»spanä¸­æå–æ ‡é¢˜
    span_element = pub_element.find('span')
    if span_element:
        title = extract_title_from_element(span_element)
        if title:
            shortname = extract_shortname_from_element(full_text, title)  # ä½¿ç”¨å®Œæ•´æ–‡æœ¬æå–shortname
            return title, shortname

    return None, None

def extract_title_from_element(element):
    """
    ä»ç»™å®šå…ƒç´ ï¼ˆfontæˆ–spanï¼‰ä¸­æå–æ ‡é¢˜ï¼š
    1. å¦‚æœå­˜åœ¨aæ ‡ç­¾ï¼Œä½¿ç”¨aæ ‡ç­¾çš„æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
    2. å¦‚æœä¸å­˜åœ¨aæ ‡ç­¾ï¼Œä½¿ç”¨å€’æ•°ç¬¬äºŒä¸ªé€—å·å’Œæœ€åä¸€ä¸ªé€—å·ä¹‹é—´çš„å†…å®¹ä½œä¸ºæ ‡é¢˜
    """
    if not element:
        return None

    # é¦–å…ˆæŸ¥æ‰¾aæ ‡ç­¾
    a_tag = element.find('a')
    if a_tag:
        title = a_tag.get_text().strip()
        title = title.rstrip('.') # å»æ‰æœ«å°¾çš„å¥å·
        title = replace_special_chars(title)
        return title

    # å¦‚æœæ²¡æœ‰aæ ‡ç­¾ï¼Œä½¿ç”¨é€—å·åˆ†éš”æ³•
    contents = [str(content) for content in element.contents]
    text = ''.join(contents)

    comma_positions = [i for i, char in enumerate(text) if char == ',']
    if len(comma_positions) < 2:
        return None

    # è·å–å€’æ•°ç¬¬äºŒä¸ªå’Œæœ€åä¸€ä¸ªé€—å·çš„ä½ç½®
    last_comma = comma_positions[-1]
    second_last_comma = comma_positions[-2]

    # æå–æ ‡é¢˜å¹¶æ¸…ç†
    title = text[second_last_comma + 1:last_comma].strip()
    title = replace_special_chars(title)

    return title

def main():
    # è®¾ç½®è·¯å¾„é…ç½®
    base_dir = 'python/'
    pdf_dir = os.path.join('assets', 'pdf')
    rst_dir = os.path.join(base_dir, 'rsts')

    bib_library = os.path.join(base_dir, '505.bib')
    bib_matched = os.path.join("_bibliography", 'papers.bib')

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs(pdf_dir, exist_ok=True)
    os.makedirs(rst_dir, exist_ok=True)

    # Load existing bib entries
    bib_entries = load_bib_entries(bib_library)

    # Initialize result containers
    matched_with_pdf = []      # Case 1: åŒ¹é…åˆ°ä¸”æœ‰PDF
    matched_no_pdf = []        # Case 2: åŒ¹é…åˆ°ä½†æ²¡æœ‰PDF
    unmatched_with_pdf = []    # Case 3: æœªåŒ¹é…åˆ°ä½†æœ‰PDF
    unmatched_no_pdf = []      # Case 4: æœªåŒ¹é…åˆ°ä¸”æ²¡æœ‰PDF
    skipped_entries = []       # è·³è¿‡æ¡ç›®çš„åˆ—è¡¨

    # Scrape website
    base_url = 'https://www.ruizhang.info/publications/'
    url = base_url + 'pubindex.htm'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    publications = soup.find_all('li')[:212]

    matched_entries = []  # ç”¨äºæŒ‰é¡ºåºå­˜å‚¨åŒ¹é…çš„æ¡ç›®

    # Add progress bar
    for pub in tqdm(publications, desc="Processing publications", unit="paper"):
        title,shortname = extract_paperinfo_from_website(pub)
        if not title:
            skipped_entries.append({'html': str(pub), 'title': '{title}', 'shortname': '{shortname}'})
            continue

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰PDF
        pdf_found = False
        pdf_href = None
        links = pub.find_all('a')
        for link in links:
            href = link.get('href', '')
            if href and href.lower().endswith('.pdf'):
                pdf_found = True
                pdf_href = urljoin(base_url, href)
                break

        # 2. æ£€æŸ¥æ˜¯å¦èƒ½åŒ¹é…åˆ°æ¡ç›®
        matching_entry = None
        for entry in bib_entries:
            if fuzzy_match_title(entry.get('title', ''), title):
                matching_entry = entry
                break

        # 3. æ ¹æ®åŒ¹é…ç»“æœå’ŒPDFå­˜åœ¨ä¸å¦åˆ†ç±»å¤„ç†
        if matching_entry:
            if pdf_found:
                # Case 1: åŒ¹é…åˆ°ä¸”æœ‰PDF
                filename = matching_entry.get('doi', '') or matching_entry.get('ID', '')
                if download_pdf(pdf_href, filename, pdf_dir):
                    matching_entry['pdf'] = f"{filename}.pdf"
                matching_entry['shortname'] = shortname
                matched_with_pdf.append(matching_entry)
                matched_entries.append(matching_entry)
            else:
                # Case 2: åŒ¹é…åˆ°ä½†æ²¡æœ‰PDF
                matching_entry['shortname'] = shortname
                matched_no_pdf.append(matching_entry)
                matched_entries.append(matching_entry)
        else:
            if pdf_found:
                # Case 3: æœªåŒ¹é…åˆ°ä½†æœ‰PDF
                safe_title = title.lower()  # è½¬æ¢ä¸ºå°å†™
                safe_title = re.sub(r'[^a-z0-9\s-]', ' ', safe_title)
                safe_title = ' '.join(safe_title.split())
                safe_title = safe_title.replace(' ', '-')
                safe_title = safe_title[:200]

                download_pdf(pdf_href, safe_title, pdf_dir)
                unmatched_with_pdf.append({'item': str(pub),'pdf': f"{safe_title}.pdf"})
            else:
                # Case 4: æœªåŒ¹é…åˆ°ä¸”æ²¡æœ‰PDF
                unmatched_no_pdf.append({'item': str(pub)})

    # 4. ä¿å­˜ç»“æœ
    with open(bib_matched, 'w', encoding='utf-8') as f:
        for entry in matched_entries:  # ä¿®æ”¹ï¼šä½¿ç”¨æŒ‰é¡ºåºå­˜å‚¨çš„åˆ—è¡¨
            write_bibtex_entry(f, entry)

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    with open(os.path.join(rst_dir, 'summary.txt'), 'w', encoding='utf-8') as f:
        f.write("### ç»Ÿè®¡ä¿¡æ¯ ###\n")
        f.write(f"æ€»å…±å¤„ç†æ–‡çŒ®æ•°: {len(publications)}\n")
        f.write(f"æ¡ç›®åŒ¹é…åˆ°ä¸”ç½‘ç«™å­˜åœ¨PDF: {len(matched_with_pdf)}\n")
        f.write(f"æ¡ç›®åŒ¹é…åˆ°ä½†ç½‘ç«™æ— PDF: {len(matched_no_pdf)}\n")
        f.write(f"æ¡ç›®æœªåŒ¹é…ä½†ç½‘ç«™æœ‰PDF: {len(unmatched_with_pdf)}\n")
        f.write(f"æ¡ç›®æœªåŒ¹é…ä¸”ç½‘ç«™æ— PDF: {len(unmatched_no_pdf)}\n")
        f.write(f"è·³è¿‡æ¡ç›®: {len(skipped_entries)}\n")

    # ä¿å­˜åŒ¹é…åˆ°ä½†ç¼ºå°‘PDFçš„è­¦å‘Š
    if matched_no_pdf:
        with open(os.path.join(rst_dir, 'matched_no_pdf.txt'), 'w', encoding='utf-8') as f:
            f.write("âš ï¸ åŒ¹é…åˆ°ä½†ç¼ºå°‘PDFçš„æ¡ç›®:\n\n")
            for entry in matched_no_pdf:
                f.write(f"- Title: {entry.get('title', 'No title')}\n")
                f.write(f"  ID: {entry.get('ID', 'No ID')}\n\n")

    # ä¿å­˜æœªåŒ¹é…ä½†æœ‰PDFçš„è­¦å‘Š
    if unmatched_with_pdf:
        with open(os.path.join(rst_dir, 'unmatched_with_pdf.txt'), 'w', encoding='utf-8') as f:
            f.write("â— æœªåŒ¹é…ä½†å­˜åœ¨PDFçš„æ¡ç›®:\n\n")
            for entry in unmatched_with_pdf:
                f.write(f"- Item: {entry['item']}\n")
                f.write(f"  PDF: {entry['pdf']}\n\n")

    # ä¿å­˜æœªåŒ¹é…ä¸”æ— PDFçš„è­¦å‘Š
    if unmatched_no_pdf:
        with open(os.path.join(rst_dir, 'unmatched_no_pdf.txt'), 'w', encoding='utf-8') as f:
            f.write("âŒ æœªåŒ¹é…ä¸”æ— PDFçš„æ¡ç›®:\n\n")
            for entry in unmatched_no_pdf:
                f.write(f"- Item: {entry['item']}\n\n")

    if skipped_entries:
        with open(os.path.join(rst_dir, 'skipped_entries.txt'), 'w', encoding='utf-8') as f:
            f.write("ğŸš« è·³è¿‡æ¡ç›®:\n\n")
            for entry in skipped_entries:
                f.write(f"- Title: {entry['title']}\n")
                f.write(f"  Shortname: {entry['shortname']}\n")
                f.write(f"  HTML: {entry['html']}\n\n")

if __name__ == "__main__":
    main()